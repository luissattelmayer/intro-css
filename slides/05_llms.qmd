---
title: "Large Language Models"
author : "Malo Jan & Luis Sattelmayer"
date : today
execute: 
  echo: false
  warning: false
format: 
    revealjs:
        highlight-style: breeze
        theme: simple
        pdf-separate-fragments: false
        self-contained: true
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

## Climbing the ladder of abstraction in NLP {.smaller}
- Bag of Words
    - text is represented as a collection of individual words without considering order or context
    
- Word embeddings
    - dense, fixed-size vectors capturing word semantics
    - e.g. Word2Vec, GloVe, FastText

- Today: **Large Language Models** (LLMs)
    - context-aware embeddings using attention mechanisms
    - e.g. BERT, GPT, Llama & Co
    
## {.smaller .scrollable}

| **Feature**               | **Bag-of-Words**                                           | **Static Embeddings**                                    | **Transformers**                                     |
|---------------------------|-----------------------------------------------------------|----------------------------------------------------------|-----------------------------------------------------|
| **Core Idea**             | Word count representation                                | Dense, fixed-size vectors capturing word semantics.      | Context-aware embeddings using attention mechanisms |
| **Representation**        | Sparse vectors (e.g., one-hot encoding, counts)          | Dense vectors (e.g., 300 dimensions).                   | Contextualized vectors generated dynamically       |
| **Semantics**             | None (words treated independently)                      | Semantic similarity but context-independent.            | Semantic and context-aware understanding           |
| **Polysemy Handling**     | Cannot distinguish between meanings                     | Single vector for all senses of a word.                 | Context-aware disambiguation                       |
| **Methods**               | TF-IDF, word counts, scaling methods                    | Word2Vec, GloVe, FastText.                              | LLMs: BERT, GPT, Llama & Co                        |
| **Weaknesses**            | Ignores order and context, sparse                       | Context-blind, limited for nuanced tasks.               | Computationally expensive, requires large data     |




## Large Language Models
![](images/05_llms_overview.png)

## 

![](images/05_history_of_ai.png)


## 

![](images/05_layers_of_ai.png)


## Neural Networks
![](images/05_geoffrey_hinton.jpg)

- A profile in the [New Yorker](https://www.newyorker.com/magazine/2023/11/20/geoffrey-hinton-profile-ai) about Geoffrey Hinton 


## Neural Networks {.smaller}
- Computational model inspired by the way biological neural networks in the human brain process information
- Designed to recognize patterns, make predictions, or classify data by mimicking the interconnected structure of neurons in the brain

![A simple one-layered NN](images/05_neural_network.png)


## Deep Learning {.smaller}
- Oftentimes deep learning & NNs used interchangeably
- Deep refers to the number of layers in the network (i.e. the depth)

- **Weights**: values that define the strength of the connection between two neurons in adjacent layers of a neural network
- **Biases**: parameters that allow the network to shift the activation function up or down
- **Activation Function**: decides whether a neuron should "fire" (pass its signal forward) and how strong that signal should be
    - necessary to introduce *non-linearity* into the network and handle complex patterns in the data


## Transformer Architecture 
- Also a NN, state of the art since early 2018
- "Attention is all you need" [@vaswani2017attention]

- **Attention Mechanism**: 
    - allows the model to focus on different parts of the input sequence when predicting the next token
    - crucial for understanding the *context* of a word in a sentence

## Attention in Transformers
- For a visualization see [here](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing)
- Viz below taken from @clark2019does

![](images/05_attention_mechanism.png)




## Training a transformer
- After the model was given huge amounts of raw text, it learns *causal language modeling* and *masked language modeling*

![Causally predicting the next word](images/05_causal_modeling.png)


## Masked Language Modeling

![](images/05_masked_modeling.png)

## Transfer learning & fine-tuning {.smaller}

- LLMs are trained on large corpora and thus contain a lot of knowledge about the (written) world
- This knowledge can be *transferred* to other similar and related tasks
- Transfer learning adds a new layer of neural network on top of the pre-trained model


:::{.fragment}
- *Fine-tuning* a pre-trained model on a specific task is often faster and requires less data than training a model from scratch
- The model is fine-tuned on a smaller dataset to adapt to the specifics of the new task
- For fine-tuning, The weights within the model are adjusted to the new task
:::

## Transfer learning
![](images/05_transfer_learning.png)



## From pretrained models...
![](images/05_pretrained_model.png)


## ... to finetuning
![](images/05_finetuned_model.png)

## Finetuning visualized [@do2024augmented]

![](images/03_do_supervised.png)




## 

![](images/05_transformer_architecture.png)

## 

![](images/05_nvidia_stock.png)

## Nvidia stock development 
![](images/05_nvidia_stock.png)


## What is GPU? {.smaller}

- **Graphics Processing Unit**: 
    - a specialized processor to render images and video
    - parallel processing capabilities
    - complements the CPU (Central Processing Unit), which excels at sequential tasks
    - optimized for matrix and vector operations; crucial for neural network computations


- **CUDA**: Compute Unified Device Architecture
    - a parallel computing platform and application programming interface model created by NVIDIA
    - allows software developers to use a CUDA-enabled GPU for general purpose processing

## GPU for LLMs
- Massive computation requirements
- Memory bandwidth
- Parallelization
- A task that would take days with a CPU can be done in hours with a GPU
- This does mean that working with LLMs...
    - ... requires patience, time, and unfortunately computational ressources




## BERT {.smaller}
- **B**idirectional **E**ncoder **R**epresentations from **T**ransformers
    - masked language model for NLP tasks
    - only uses encoder part of the architecture
    - bi-directional attention, meaning it looks at the context from both the left and right sides of a word

- **RoBERTa**: Removed NSP and trained on larger datasets with more robust techniques
- **DistilBERT**: Smaller and faster version of BERT
- and other variants trained on different languages, tasks, etc 


## BERT vs. GPT {.smaller}

|    | **BERT**                             | **GPT**                             |
|-------------------|--------------------------------------|--------------------------------------|
| **Type**         | Encoder-only                        | Decoder-only                        |
| **Training**    | Masked Language Modeling (MLM)       | Causal Language Modeling (CLM)      |
| **Direction**    | Bi-directional                      | Uni-directional                     |
| **Task Focus**   | Language understanding              | Language generation                 |
| **Strength**     | Deep sentence/context understanding | Coherent and fluent text generation |


## Evolution of LLMs
![](images/05_evolutionary_tree.jpg)
## Multilinguak Models
![](images/05_multilingual_embeddings.jpg)

## Multilingual Models
![](images/05_translation.png)



## Issues with LLMs {.smaller}
- Carbon footprint
    - on quantifying ML's carbon footprint see @lacoste2019quantifying

- LLMs hardly solve the problem of bias in AI
- GPU dependent
- Data privacy:
    - without personal GPU, you will have to think a lot about your data and whether it ought to be protected
    - some corpora (e.g. copyrighted data) should not be seen by third parties (Google Colab)
- What is the training data? Where does it come from?


## Poli-sci examples
- Measure populist frames in text: @bonikowski2022politics
- Anti-elite rhetoric: @licht2024measuring
- Social groups: @licht2024they

## What other use-cases for Transformers? 
- This leaves the realm of textual analysis
- Image generation
    - Dall-E
    - Midjourney
    - Stable diffusion
    
- speech-to-text
    - [Whisper](https://github.com/openai/whisper)
- Machine translation
    - [NMT](https://github.com/UKPLab/EasyNMT)

## Whisper
- For the GitHub tutorial click [here](https://github.com/openai/whisper)

![Developed by OpenAI](images/05_whisper.png)

# Further ressources
- [Lena Voita's blog on NLP](https://lena-voita.github.io/nlp_course.html)
- [Natural Language Inference]() (NLI)
    - paper by @laurer2024less
    - GitHub tutorial [here](https://github.com/MoritzLaurer/summer-school-transformers-2023/blob/main/4_tune_bert_nli.ipynb)


## Bibliography





