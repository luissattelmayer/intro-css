---
title : "Introduction to supervised text classification"
author : "Malo Jan"
date : today
css: styles.css  # Link to your custom CSS file
execute: 
  echo: false
  warning: false
format: 
    revealjs:
        highlight-style: breeze
        theme: simple
        pdf-separate-fragments: false
        self-contained: true
bibliography : references.bib
footer : "Supervised text classification"
editor_options: 
  chunk_output_type: console
---

## Overview {.smaller}

- Last two days : how to get text data and obtain numerical representations of texts
- Today : how to use these numerical representations to automate the classification of texts into categories, one of the main use cases of text analysis in social sciences
- Supervised text classification : what is it, how to do it, how to evaluate it

<!-- A voir si inclusion dictionnary comme une faÃ§on de text classification-->
# Supervised text classification : uses and workflow

## Text classification as measurement in social sciences {.smaller}

- Long tradition in social sciences to measure/operationalize concepts from text
- Content analysis : assign predefined categories to texts units to measure a social phenomena  [@krippendorff2018content]
- Systematic measurement of (latent) concepts in texts, through coding rules and quantification
- Different from more qualitative/intepretative approaches of text analysis which focus on meaning, context, interpretation

## Text classification through manual content analysis {.smaller}

- Historical use through *manual content analysis*
- Use of (multiple) Experts/RAs/crowdworkers to code texts 
- Exemples : 
    - Coding of news articles to measure media frames
    - Protest event analysis :[eg. Poldem](https://poldem.eui.eu/data-overview/)
    - Extensive datasets with high number of categories : [Comparative manifesto project](https://manifesto-project.wzb.eu/), [Comparative Agendas projects](https://www.comparativeagendas.net/) 

<!-- categories are often mutually exclusive-->

## Advantages and limits of manual content analysis {.smaller}

- Importance of human judgment, context-knowledge
- High quality data, in comparison to automated methods such as dictionnaries

- Highly time consuming, human labor intensive and costly
    - Eg : Manifesto Project, CAP
- Often need to rely on a small sample of texts, which can be biased
    - Eg. Protest event Analysis

<!--
Compare to dictionnary : context of words matters
-->    

## Let the machine come in {.smaller}

- Text classification can be automated with supervised machine learning
- One of the main use cases of text analysis in social sciences
- "Augment" the human, does not replace it
- Rather than coding all the texts manually, we can train a model to predict the categories based on a sample that we have coded 
- *Supervised* : we need a labeled dataset to train the model
- *Unsupervised* : we do not need a labeled dataset, the model will learn the categories by itself
- The model will learn the text features that are associated with the categories


## Some common machine learning terms explained {.smaller}

| Machine Learning Lingo         | Statistics Lingo                          |
|---------------------------------|-------------------------------------------|
| Feature                         | Independent variable                      |
| Label                           | Dependent variable                        |
| Labeled dataset                 | Dataset with both independent and dependent variables |
| To train a model                | To estimate                               |
| Classifier (classification)     | Model to predict nominal outcomes         |
| To annotate                     | To (manually) code (content analysis)     |

: From [van Atteveldt et al, 2022](https://v2.cssbook.net/content/chapter08#tbl-mllingo)

## Inputs and outpus in supervised learning for text analysis {.smaller}

<!-- "Train a classifier" -->

- Input : text features + labels
- Model : a function that will learn the relationship between the text features and the labels
- Ouput :
    - Probabilities of belonging to each category  
    - If two categories, a binary classification, usually > 0.5
    - If more than two categories, a multi-class classification, class assign is highest probability

## Uses cases  {.smaller}

- Policy issue classification [@burscher2015using ;  @Burst2024]
- Populism, Nationalism, Authoritarianism [@licht2024measuring ; @bonikowski2022politics]
- Mentions of social groups [@licht2024they]
- "Off the record" statements in mainstream media [@do2024augmented]
- Nostalgia in party manifestos [@muller2024nostalgia]
- Stance detection : eg. towards Donald Trump [@burnham2024stance]
- In your daily life : spam detection, credit card fraud detection, content moderation on social media, sentiment analysis of Amazon reviews, language detection

## Supervised text classification pipeline {.smaller}

1. Getting a clean corpus of labelled texts
2. Transforming texts into numerical features
3. Model training
4. Model evaluation
5. Inference on full corpus
6. Validation
7. Use in downstream analysis

<!--1. Create a training set
1. Get a clean corpus of texts segmented into units (eg. documents, sentences, paragraphs)
2. Get labelled data
    - Sample a subset of the corpus to annotate
    - Set coding rules, annotate the sample + refine codebook
3. Get text features
    - Tokenize the texts + preprocess
    - Convert into numerical features (BOW, TF-IDF, word embeddings, BERT, etc.)
4. Train a model
    - Split the annotated sample into a training set and a test set (70/30 rule)
    - Choose a model (logistic regression, SVM, random forest, neural network, transformer)
    - Train a model on the training set based on text features and labels
5. Evaluate the model
    - Evaluate the model on the test set
    - Compute performance metrics (accuracy, precision, recall, F1-score)
6. Infer the model
    - Apply the model to the rest of the corpus
7. Validation
8. Use in downstream analysis-->


## {.smaller}

![@licht2024measuring : A supervised learning workflow](images/03_licht_pipeline.png)

## {.smaller}

![@do2024augmented : Policy vs Politics classification task](images/03_do_supervised.png)

# Getting a clean corpus of labelled texts

## Getting a clean corpus of labelled texts {.smaller}

- Training a "classifier" requires labelled data : texts with categories assigned
- Labels are used to :
    - Train the model
    - As a gold standard to evaluate the performance of the model
- Existing labels
    - Potential labels with similar categories from other projects
    - Labels in the wild : metadata from datasets/website
- Most of the time, we need to label the data ourselves

## *Which* texts to annotate ? {.smaller}

- From a corpus of texts, we need to select a sample to annotate
- Series of decisions to make
- Text unitization : documents, paragraphs, sentences, tokens ? 
- Sample size : how many texts to annotate ?
    - Usually the more the better, to a certain extent
    - But depends on number of categories, complexity of the task
- Sampling strategy : random, stratified ?
    - Important to have a representative sample 
- Active learning : select algorithmically the most informative texts to annotate



## *How* to annotate ? {.smaller}

- Coding rules, codebook, to be refined
- Quality of annotation is super important : garbage in, garbage out
- Has to be systematic, replicable
- Get to know the data
- Ideal case (often needed to publish) : several annotators, inter-coder reliability tests (Cohen's Kappa, Krippendorf alpha, etc.)

## *Who* annotates ? {.smaller}


:::: {.columns}
::: {.column width="50%"}


- Most of text classification projects rely on RAs or crowdworkers
    - Trained on the codebook and paid for their work
- As phd students, we often do it ourselves
    - Time consuming, but we know the data better
:::

::: {.column width="50%"}

![](https://dagshub.com/blog/content/images/2022/06/Grad-Student.jpg)
![](images/03_benoit.png)

:::
::::

## Can LLms annotate for you ? {.smaller}

:::: {.columns}
::: {.column width="50%"}


![@gilardi2023chatgpt](images/03_gilardi.png)

![@ollion2023chatgpt](images/03_ollion.png)

:::

::: {.column width="50%"}

![@tan2024large](images/03_tan.png)

![@alizadeh2025open](images/03_gilardi2.png)
:::
::::




# Transforming texts into numerical features

## Transforming texts into numerical features {.smaller}

:::: {.columns}
::: {.column width="50%"}


- Texts are not directly usable by machine learning algorithms
- Need to convert them into numerical features
- Different possibilities : 
    - Bag-of-words (BOW)
    - Term frequency-inverse document frequency (TF-IDF)
    - Static word embeddings (Word2Vec, GloVe) 
    - Contextual word embeddings (eg. BERT)
:::

::: {.column width="50%"}


![](images/03_voita.png)

:::
::::



# Model training

## Supervised learning models {.smaller}

- Split the annotated sample into a training set and a test set (70/30 rule)
- A supervised learning model is a model that learns a mapping function from input features to output labels based on a training set
- The model "learns" the underlying relationship between the input features and the labels by minimizing a loss function that measures the difference between the predicted labels and the true labels
- During the training phase, the model adjusts its parameters to minimize the loss function and make better predictions
- Models differ in the way they learn this mapping function
- Function then used to predict the labels of the test set
- Return the probability of a text belonging to a category

## Different types of models {.smaller}


- Classical models : Logistic regression, Naive Bayes, SVM, Random Forest
    - Simple and fast
    - Rely on bag-of-words representations
    - Lot of preprocessing needed
    - Limited performance 

- Transformers models (eg: BERT)
    - Contextual representation of texts
    - No need for extensive preprocessing
    - Transfer-learning paradigm : models have already language knowledge
    - Computationally intensive

# Model evaluation

## Evaluating performance of the model {.smaller}

- Use the test set to evaluate the performance of the model
    - Set of texts that the model has not seen but for which we know the categories
    - "Held-out" data/Gold standard
- The model is used to predict the categories of the texts in the test set
- The predictions are compared to the true categories with different metrics

- **Accuracy** : proportion of correctly classified texts (highly limited for imbalanced datasets)


$$
\
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
\
$$

## Confusion matrix {.smaller}

- Positive class : the class we want to predict
- Negative class : the other class

| | Predicted Positive | Predicted Negative |
|---|---|---|
| **Actual Positive** | True Positive (TP) | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN) |

## Recall, Precision and f1-score {.smaller}

- Recall : proportion of actual positive cases that were correctly classified

$$
\
\text{Recall} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Negative}}
\
$$

- Precision : proportion of predicted positive cases that were correctly classified

$$
\
\text{Precision} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Positive}}
\
$$

- f1-score : [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of recall and precision

$$
\
\text{f1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\
$$

## Recall and precision trade-offs {.smaller}

- Recall and precision are often in trade-off
    - Increasing recall involves predicting more positive cases at the risk of increasing false positives (reduce precision)
    - Increasing precision involves predicting less positive cases at the risk of missing some positive cases (reduce recall)
- The model may be optimized to maximize one of the two, depends on the task
    - Eg. in a spam detection task, we want to maximize precision : we prefer to have some spam in our inbox rather than missing an important email
    - Content moderation/hate speech : we often want to maximize recall : we prefer to have some false positives rather than missing a hate speech
- We can change the threshold of the model to optimize recall or precision (ROC)

## What is a good performance ?  {.smaller}

- Maximization of one metric, but depends on the task
- In general, we look for at least f1-score > 0.7
- A "good" performance depends on the complexity of the task
    - Conceptual complexity 
    - Task complexity : multi-class, imbalanced classes
- Performance also vary across class
- But, sometimes we do not want our model to be too good neither : overfitting 

## How to improve performance {.smaller}

| Problem                       | Solution                        |
|-------------------------------|---------------------------------|
| Unbalanced classes            | Undersampling & oversampling    |
| Not enough training data      | More annotation                 |
| Bad quality of the training data | Better annotation             |
| Bad quality of the text features | Better preprocessing          |
| Limited text representation   | Go for more complex models      |
| Too complex concept               | Accepting okay-ish performance  |
 
# Inference and validation

## Inference {.smaller}

- Once a model is trained and evaluated, it can be used for inference
- For a supervised learning model, the inference is the prediction of the categories for new (unseen) texts
- Corpus of 1 million texts, model trained on 1000, use the model to classify all the remaining texts
- Important to consider the generalization capacity of the model on new data
    - Out-of-domain texts : texts that are very different from the training texts
    - Out-of-language texts : texts in a language different from the training language
- If texts are a bit different, may be important to compare predictions with human annotations

## Measurement validity {.smaller}

- Face validity : does the measure look like it is measuring what it is supposed to measure ?
- Convergence validity : to what extent our measure is correlated with other measures of the same concept ? 

![Convergence validy of measure of populism](images/03_licht_ches.png)

## Benchmark with other methods {.smaller}

- In addition to human annotation, may be important to compare the performance of the model with other methods such as :
    - Other supervised learning models
    - Dictionary-based methods


# What to do with classification outputs ? 
## What to do classification outputs ? {.smaller}

- It is (mostly) all about measurement of previously unmeasured concepts
- 4 main uses of classification outputs :
    - Descriptive variation of concept prevalence across time, countries, groups
    - Use of prediction quality as a measure of concepts
    - Use of classification output as dependent variable in a regression
    - Use of classification output as independent variable in a regression

## Descriptive variation of concept prevalence {.smaller}

![@do2024augmented](images/03_do_politics.png)

## Use prediction quality as a measure of the concept {.smaller}

<br>


![@peterson2018classification : accuracy as a measure of polarization](images/03_peterson.png)




## Classification output as DV  {.smaller}

![@licht2024measuring : predicting the use of anti-elite strategies](images/03_licht_populism.png)

## Classification output as IV {.smaller}

![Sattelmayer forthcoming : the effect of party position on immigration on vote switching to the far right](images/03_sattelmayer.png)

## References

