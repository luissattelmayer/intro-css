---
title: "Corpus selection and online data collection"
author: "Malo Jan & Luis Sattelmayer"
date: today
format: 
  revealjs:
    highlight-style: breeze
    theme: simple
    pdf-separate-fragments: false
    self-contained: true
    slide-number: c/t
    footer: "Collecting online data"
    theme: simple
    echo: true
    incremental: false
bibliography: references.bib
---

# Corpus selection

## What is a corpus ? {.smaller}

- Every project involving text analysis starts with a corpus 
- A collection of *machine readable* texts, along with metadata
- Corpus selection is a crucial task and often overlooked
- Corpus selection is sampling : identifying the population of interest
- Corpus selection is case selection : choosing documents to analyze according to research questions and interests
- What would be a corpus that would help me answer my research question ?
    - E.g.: Twitter data is fun but not useful for everything


## Corpus selection and bias {.smaller}

- Several aspects to consider when selecting a corpus
- How exhaustive is the corpus ?        
    - Can I obtain the entirety of a document type/collection?
- Think about data generation process
    - How were these texts produced?
    - How could this affect the corpus?
- Important to explore, read some of the texts
- Think about quantity of interest,  what could you derive as a measure from those texts : 
    - eg. How much a given topic is discussed in these texts
    - eg. How much a given sentiment is expressed in these texts
    
## Four biases in text selection {.smaller}
- **Resource Bias**
  - "texts often better reflect populations with more ressources to produce, record, and store documents" [@grimmer2022text]
  - social groups are marginalized and invisible in all societies, thus also in text
  - eg. Twitter data is not representative of the general population; it can only speak for behavior *on Twitter*

- **Incentive Bias**
  - (self-)censoring on Social Media or in official government documents is an issue [@king2013censorship]
  
- **Medium Bias**
  - Twitter only allowed 140 characters until 2017 or meeting transcripts/speeches do not necessarily reflect the tonality of the actual communication

- **Retrieval Bias**
  - errors or skewed queries in text mining, intransparent APIs

    
## Acquiring a corpus {.smaller}

- Existing corpora vs creating original corpora
- Existing corpora 
    - Texts available in open data : eg. [French parliament speeches](https://data.assemblee-nationale.fr/travaux-parlementaires/debats) 
    - Texts released by other researchers : eg. [Manifesto corpus](https://manifesto-project.wzb.eu/information/documents/corpus), [UNGA corpus](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/0TJX8Y), [Parlspeech](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/L4OAKN), [EU commissioners press releases](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/UGGXUF), SOTU corpus

- Collecting digital texts
    - Image to text : digitization of images  through Optical Character Recognition (OCR) 
    - Audio/video to text : automatic transcription
    - Application programming interface (API)
    - Web scraping


# Transforming videos, audios and images into text

## Transcribing audio and video files {.smaller}

- A lot of data is in audio/video format such as interviews, radio shows, speeches, podcasts, etc.
- This data can be analyzed with text analysis tools through transcription
- Potential pipeline : download videos from youtube and transcribe audio with whisper

## Corpus creation with digitized texts {.smaller}

- Important avenues for research in the digital humanities
- Digitization of old texts, newspapers, books, archives
- Allows to create large original corpora
- Requires OCR (Optical Character Recognition) tools
    - Transforms images of texts into machine-readable text
    - Tools such as [Tesseract](https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html), [TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)
    
# APIs

## Application programming interface (API) {.smaller}

:::: {.columns}
::: {.column width="40%"}


- One important way to access data online
- Access to data through
    - Authentication with an API key
    - Querying the API with specific requests
    - Receiving data in a structured format (JSON, XML) 

:::

::: {.column width="60%"}

![](https://media.geeksforgeeks.org/wp-content/uploads/20230216170349/What-is-an-API.png)
:::
::::



## Advantages of APIS {.smaller}

- Allows for easy & legal access to data
- Get structured data that can be easily processed
- In many cases, someone has already created a package facilitating the access to the API :  [NY Times](https://github.com/news-r/nytimes), [Manifesto project](https://manifesto-project.wzb.eu/information/documents/api), [Bluesky](https://github.com/JBGruber/atrrr),  [Spotify](https://www.rcharlie.com/spotifyr/), [Youtube](https://developers.google.com/youtube/v3?hl=fr)  [Twitter (RIP)](https://github.com/cjbarrie/academictwitteR)

## Limits of APIs {.smaller}

- Access only to the data that the API provider wants to share
- Often limited in the number of requests you can make
- Some APIs are not free : eg. Twitter API now
- APIcalypse/Post-API Age : APIs can be closed or modified at any time
    - Facebook, or Twitter that had before extensive API access for researchers but now closed (see [minet](https://github.com/medialab/minet) for some workarounds)


## Learning how to use an API {.smaller}

- This course will not cover how to use APIs
- But there are many resources online, [here](https://tellingstorieswithdata.com/07-gather.html)
- Request data from API with [httr2](https://www.tidyverse.org/blog/2023/11/httr2-1-0-0/)
- Parse Json or XML in R with [jsonlite](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html) 


    
# Webscraping

## What is web scraping? {.smaller}

<br>

::: {.fragment fragment-index=1}

> Web scraping is a data collection technique on the web that involves extracting data from a web page.

:::

::: {.fragment fragment-index=2}

- We are all manual web scrapers: copying/pasting/downloading data from the web daily.
- But we can automate all of this: machines are faster and more extensive than we are.
:::

## Use cases {.smaller}

- Collecting social media data
- Collecting press releases from an organization, speeches from actors
- Extracting data from Wikipedia pages
- Automatically download hundreds of PDFs from a website
- Collect metadata from a website

## How the web is written {.smaller}

- Web scraping involves a minimum understanding of how the web is written: what is a web page?
    - Code: interpreted by a browser (ex: Chrome, Mozilla)

- The code of a web page can be written in:
    - HTML (**Hypertext markup language**): structure and content
    - CSS: style (ex: font, color)
    - Javascript: functionalities, dynamic content, search, drop-down menus etc.

- Exemple from the press release of the [UN Secretary General](https://press.un.org/en/content/secretary-general/press-release)

## HTML {.smaller}

- HTML is a markup language
- The content of the web is written in tags, which can have attributes
- Most common tags in html
    - div
    - p
    - h1, h2, h3

- Web scraping consists in extracting the content of the html source code to get certain information
- CTRL + U
- [Selector Gadget](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=de&pli=1)


## Ethics on data access {.smaller}

- Web scraping is about collecting data not intended for you
    - May be illegal but also exceptions for [research purposes]((https://www.ouvrirlascience.fr/la-fouille-de-textes-et-de-donnees-a-des-fins-de-recherche-une-pratique-confirmee-et-desormais-operationnelle-en-droit-francais/))
    - Access to private data, overload servers
    - Websites can block you/track you

- Good practices :
    - API first
    - Check permissions (robot.txt)
    - Slow down scraping

## Ethics on data use {.smaller}

:::: {.columns}
::: {.column width="60%"}

- Web scraping can be used to collect personal and sensitive data
- E.g.: Old twitter API: possibility to download all tweets of a user in 2 lines of code
- Data protection laws (GDPR) apply to web scraping
- Need to think about the use of the data

:::

::: {.column width="40%"}

![](images/01_scrap.png){.absolute top=100 right=10 width="400" height="400"}
:::
::::

## Complex and dynamic pages {.smaller}

:::: {.columns}
::: {.column width="60%"}

- Web pages are sometimes complex and dynamic (javascript)
- More complex scraping strategies are needed
- Selenium: allows to simulate clicks on a browser from a script
- Web pages are also changed, updated: your program can work one day and not the next, good practice to download the hmtl pages on your computer

:::

::: {.column width="40%"}
![](images/01_scrap_2.jpg){.absolute top=100 right=0 width="300" height="300"}
:::

::::




## References




