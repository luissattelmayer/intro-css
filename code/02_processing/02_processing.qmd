---
output: html_document
editor_options: 
  chunk_output_type: console
---

- Tidytext
- But also quanteda

```{r}
library(tidytext)
library(tidyverse)
library(here)
library(stopwords)
```

```{r}
unsg <- read_csv(here("data/unsg_dataset.csv"))

unsg
```

## Processing text with stringr

In R : text is represented as a string of characters : character vectors. 

```{r}
text <- "Donald Trump  has been elected   president in 2016, Biden in 2020 and we are in 2025 with Donald Trump again.   "

class(text)
```



```{r}
str_length(text)
str_c(text, " Let's see what happens this year.")
str_sub(text, start = 1, end = 5)
str_sub(text, start = -12, end = -1)
str_detect(text, "Trump")
str_replace(text, "Trump", "J Trump")
str_replace_all(text, "Trump", "J Trump")
str_remove(text, " Trump")
str_remove_all(text, " Trump")
str_to_lower(text)
str_to_upper(text)
str_to_title(text)
str_squish(text)
```

## Regular expressions







## Tokenization

```{r}
unsg_tokens <- unsg |> 
  unnest_tokens(word, text, token = "words")

unsg_sentences <- unsg_tokens |> 
  unnest_tokens(sentence, sentence, token = "sentences")

unsg_tokens |> 
    count(word, sort = TRUE)
```

- Remove full stops, punctuation, put to lower case

## Stopwords

```{r}
english_stopwords <- stopwords::stopwords("en")
# for french stopwords::stopwords("fr"), german stopwords::stopwords("de") etc. 

unsg_tokens <- unsg_tokens |> 
    filter(!word %in% english_stopwords)

# Vocabulary size
n_distinct(unsg_tokens$word)

# Remove digits

unsg_tokens <- unsg_tokens |> 
  filter(!str_detect(word, "\\d+"))

unsg_tokens |> 
    count(word, sort = TRUE)

unsg_tokens  |>
    count(word, sort = TRUE) |> 
    filter(n > 18000) |> 
    ggplot(aes(fct_reorder(word, n), n)) +
    geom_col() +
    theme_light() +
    coord_flip() +
    labs(title = "Most frequent words in the UNSG speeches",
         x = "Word",
         y = "Frequency")


```

## Stemming

```{r}
library(SnowballC)

unsg_tokens <- unsg_tokens |> 
    mutate(word_stem = SnowballC::wordStem(word))

unsg_tokens |> 
    count(word_stem, sort = TRUE)
```

## Lemmatization



## Word and document frequencies

Tf-idf : decrease the weight of common words and increase the weight of rare words.


```{r}
unsg_tf_idf <- unsg_tokens |> 
    count(secretary_general, word) |> 
    bind_tf_idf(word, secretary_general, n)

unsg_tf_idf

unsg_tf_idf %>%
  group_by(secretary_general) %>%
  slice_max(tf_idf, n = 20) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = secretary_general)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~secretary_general, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```


## DTM 

<!-- Collocations, bigrams and so on-->





## Stemming/Lemmatizing & Stopwords



## n_grams



## Some first BoW methods

- tf-idf 

