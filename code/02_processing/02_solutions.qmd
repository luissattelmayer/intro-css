---
title: "Solutions Day 2"
format: html
---

## Exercises

1. Import the data from this link "https://www.dropbox.com/s/dpu5m3xqz4u4nv7/tweets_house_rep_party.csv?dl=1"

2. Create a keyword dictionary yourself for abortion related issues.

3. Plot the evolution of keyword mentions over time. 

4. Subset the corpus to only include tweets that contain the keywords you have defined.

5. Find regular expressions to remove all the links, and mentions from the tweets.

6. Preprocess the corpus (tokenization, removing the stopwords, stemming).

7. Create a tf-idf per party to show which words distinguish the parties in their abortion related discourse.

8. Plot the tf-idf per party.


```{r}

needs(tidyverse, tidytext, SnowballC, stopwords)

timelines <- read_csv("https://www.dropbox.com/s/dpu5m3xqz4u4nv7/tweets_house_rep_party.csv?dl=1") |> filter(!is.na(party))

timelines
```
```{r}
# Remove links and mentions from the tweets

timelines <- timelines |> 
  mutate(text = str_remove_all(text, "http\\S+")) |> 
  mutate(text = str_remove_all(text, "@\\S+"))

timelines
```



```{r}
keywords <- c("abortion", "prolife", " roe ", " wade ", "roevswade", "baby", "fetus", "womb", "prochoice", "leak")

preprocessed <- timelines |> 
  rowid_to_column("doc_id") |> 
  filter(str_detect(text, str_c(keywords, collapse = "|"))) |> 
  unnest_tokens(word, text) |> 
  anti_join(get_stopwords()) |> 
  mutate(stemmed = wordStem(word))

keyword_counts <- timelines |> 
  rowid_to_column("doc_id") |> 
  mutate(keyword_count = str_count(text, str_c(keywords, collapse = "|"))) |> 
  unnest_tokens(word, text, drop = FALSE) |>  # Retain the original text column
  anti_join(get_stopwords(), by = "word") |> 
  mutate(stemmed = wordStem(word)) |> 
  group_by(doc_id, party, date) |>                 # Group by doc_id and party
  summarise(
    text = first(text),                      # Explicitly keep the original text
    total_keywords = first(keyword_count),    # Total count of keywords
    unique_keywords = n_distinct(stemmed)    # Count of unique stemmed keywords
  ) |> 
  ungroup()


keyword_counts |> 
    group_by(party, date) |>
    summarise(
        mean_keywords = mean(total_keywords)
    ) |>
    ggplot(aes(x = date, y = mean_keywords, color = party)) +
    # change color to red and blue
    scale_color_manual(values = c("blue", "red")) +
    geom_line() +
    facet_wrap(~party) +
    theme_minimal() 


# Calculate TF-IDF by Party
tf_idf_by_party <- preprocessed |> 
  count(party, stemmed) |>  # Count occurrences of stemmed words by party
  bind_tf_idf(stemmed, party, n) |>  # Calculate TF-IDF
  arrange(desc(tf_idf))  # Sort by TF-IDF in descending order for convenience

# Visualize TF-IDF by Party
tf_idf_by_party |> 
  group_by(party) |> 
  slice_max(tf_idf, n = 20) |>  # Take the top 20 words by TF-IDF for each party
  ungroup() |> 
  ggplot(aes(tf_idf, fct_reorder(stemmed, tf_idf), fill = party)) +
    scale_fill_manual(values = c("blue", "red")) +
  geom_col(show.legend = FALSE) +  # Bar plot without legend
  facet_wrap(~party, ncol = 2, scales = "free") +  # Separate facets for each party
  labs(
    title = "Top Words by TF-IDF for Each Party",
    x = "TF-IDF",
    y = NULL
  ) +
  theme_minimal()

```


```{r}
preprocessed
```
