---
title: "From Text to Numbers"
author: "Luis Sattelmayer"
format: html
---


```{r}
library(tidyverse)
library(tidytext)
library(here)
library(SnowballC)
library(sotu)
library(stopwords)
```

## Create a DTM

```{r}
unsg <- read_csv("https://raw.githubusercontent.com/luissattelmayer/intro-css/refs/heads/main/data/unsg_dataset.csv")

unsg
```


```{r}
sotu_raw <- sotu_meta |> 
  mutate(text = sotu_text) |> 
  distinct(text, .keep_all = TRUE)

sotu_raw |> glimpse()
```


```{r}
sotu_raw |> slice(1) |> pull(text) |> str_sub(1, 500)
```

```{r}
sotu_20cent_raw <- sotu_raw |> 
  filter(between(year, 1900, 2000))

glimpse(sotu_20cent_raw)
```

The `unnest_tokens()` function comes from the tidytext package. It splits a column into token and thus flattens the table into one-token-per-row. The tokenization is not limited to only words. You could for example tokenize into characters, n_grams, sentences, paragraphs and so on. This really depends on what your quantity of interest and analysis is. Below in `unsg_tokens`, I split the text column into words. The word argument, which comes first in the `unnest_tokens` is the name that the new column, into which our tokens will be put, is going to be called.

```{r}
toy_example <- tibble(
  text = "Look, this is a brief example for how tokenization works."
)

toy_example |> 
  unnest_tokens(output = token, 
                input = text)
```



```{r}
unsg_tokens <- sotu_20cent_raw |> 
  unnest_tokens(word, text, token = "words")
unsg_tokens |> 
    count(word, sort = TRUE)
```


Here I do the same thing but I split the text column into sentences. However, when looking at the counted output, we can see that there are issues as the `unnest_tokens()` function assumes that every dot `.` makes the end of a sentence. However, language elements like `Mr. ` will be put into a cell. 

```{r}
sotu_tokens_word <- sotu_20cent_raw |> 
  unnest_tokens(word, text, token = "words")

sotu_tokens_word |> 
    count(word, sort = TRUE)


sotu_tokens_sentence <- sotu_20cent_raw |> 
  unnest_tokens(sentence, text, token = "sentences")

sotu_tokens_sentence |> 
    count(sentence, sort = TRUE)
```


```{r}
sotu_20cent_tokenized <- sotu_20cent_raw |> 
  unnest_tokens(output = token, 
                input = text)
glimpse(sotu_20cent_tokenized)
```

## Preprocessing: Stopwords

```{r}
stopwords_vec <- stopwords(language = "en")
stopwords(language = "de") # the german equivalent
stopwords(language = "fr")
```

```{r}
# find the languages that are available
stopwords_getlanguages(source = "snowball")
```

```{r}
# find the dictionaries that are available
stopwords_getsources()
```

```{r}
sotu_20cent_tokenized_nostopwords <- sotu_20cent_tokenized |> 
  filter(!token %in% stopwords_vec)

sotu_20cent_tokenized_nostopwords$token
```


```{r}
sotu_tokens_word |> 
    count(word, sort = TRUE)

sotu_20cent_tokenized_nostopwords |> 
    count(token, sort = TRUE)
```


```{r}
sotu_20cent_tokenized_nostopwords_nonumbers <- sotu_20cent_tokenized_nostopwords |> 
  filter(!str_detect(token, "[:digit:]"))

sotu_20cent_tokenized_nostopwords
```

The corpus now contains 19263 different tokens, the so-called “vocabulary.” 1848 tokens were removed from the vocuabulary. This translates to a signifiant reduction in corpus size though, the new tibble only consists of 464271 rows, basically a 50 percent reduction. 


## Preprocessing: Stemming

```{r}
sotu_20cent_tokenized_nostopwords_nonumbers_stemmed <- sotu_20cent_tokenized_nostopwords_nonumbers |> 
  mutate(token_stemmed = wordStem(token, language = "en"))

sotu_20cent_tokenized_nostopwords_nonumbers_stemmed$token_stemmed

```


```{r}
SnowballC::getStemLanguages() # if you want to know the abbreviations for other languages as well
```



Below I also filter for words that appear less than 0.05 percent of the time. Be aware that this is an important but questionable step. Especially rare words might be very informative for the context of a word. Here, it is also to accelerate calculations below but every preprocessing step alters your corpus and the vocabulary you include in your analysis. These steps should and must be informed and reported! 

```{r}
n_rows <- nrow(sotu_20cent_tokenized_nostopwords_nonumbers_stemmed)
sotu_20cent_tokenized_nostopwords_nonumbers_stemmed |> 
  group_by(token_stemmed) |> 
  filter(n() > n_rows/2000)
```

All the things we have seen above could also simply be put into one beautiful long pipeline: 

```{r}
sotu_20cent_clean <- sotu_raw |> 
  filter(between(year, 1900, 2000)) |> 
  unnest_tokens(output = token, input = text) |> 
  anti_join(get_stopwords(), by = c("token" = "word")) |> 
  filter(!str_detect(token, "[0-9]")) |> 
  mutate(token = wordStem(token, language = "en")) |> 
  group_by(token) |> 
  filter(n() > n_rows/2000)
```


## tf-idf


Tf-idf : decrease the weight of common words and increase the weight of rare words.

```{r}
unsg_tokens <- unsg |> 
  unnest_tokens(word, text, token = "words")

unsg_tf_idf <- unsg_tokens |> 
    count(secretary_general, word) |> 
    bind_tf_idf(word, secretary_general, n)

unsg_tf_idf
```

```{r}
unsg_tf_idf %>%
  group_by(secretary_general) %>%
  slice_max(tf_idf, n = 20) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = secretary_general)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~secretary_general, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```


```{r}
sotu_tf_if <- sotu_tokens_word |> 
    filter(!str_detect(word, "[:digit:]")) |> 
    count(party, word) |> 
    bind_tf_idf(word, party, n)

sotu_tf_if %>%
  group_by(party) %>%
  slice_max(tf_idf, n = 20) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = party)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~party, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```







