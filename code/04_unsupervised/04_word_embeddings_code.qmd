---
title: "Word Embeddings"
author: "Luis Sattelmayer"
date: "2025-01-08"
format: html
---

```{r}
needs(wordsalad, tidymodels, tidyverse, textrecipes, data.table, ggrepel, tidytext, text2map, text2vec, ggrepel)
```

# Word Embeddings in R

## Measuring Similarity
There are many ways to measure the similarity between words or texts or other objects. The simplest way would probably be to take the euclidean distance between two vectors. 

$d(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^n (u_i - v_i)^2}$

However, this is not a good measure of similarity for high-dimensional data. Euclidean distances are not adapted to high-dimensional data because they are sensitive to the scale of the data. This means that longer texts will have higher euclidean distances than shorter texts, even if they are very similar. 

Instead, we can use the cosine similarity. The cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It is defined to equal the cosine of the angle between them, which is also the same as the inner product of the same vectors normalized to both have length 1.

$cos(\theta) = \frac{\mathbf{u} \cdot \mathbf{v}}{||\mathbf{u}|| \cdot ||\mathbf{v}||}$

To reformulate: the cosine similarity is the angle between two vectors in a multi-dimensional space. A cosine similarity of 1 means that the two vectors are identical, a cosine similarity of 0 means that the two vectors are orthogonal, and a cosine similarity of -1 means that the two vectors are diametrically opposed to each other. This means that if the cosine similarity of two words is close to 1, they are also closely related linguistically. A cosine similarity of 0 means that the two words are not related at all, while -1 means that they are complete opposites.

This measure is not sensitive to the scale of the data and is therefore more appropriate for high-dimensional data. Word embeddings are high-dimensional vectors that represent words in a continuous space and multi-dimensional space that has the same number of dimensions as the vectors have elements. 

```{r}
set.seed(123)

uk_manifestos <- read_csv(here::here("data/gbr_manifesto_corpus_clean.csv"))
```

## Word Similarities
The quality of any measure of distance or related calculations that have to do with word embeddings always depends on the quality of the embeddings. The quality of the embeddings depends on the quality and the quantity of the data that was used to train the embeddings. If I were to take a simple corpus of 100 documents, which are each 1000 words long, we could quickly embed 100,000 words. However, the quality of the embeddings would be very low since 100,000 words is not enough to capture the complexity of any language. 

Fortunately, there are pre-trained embeddings that we can use. Two of the most popular and most easily accessible pre-trained embeddings are the GloVe embeddings and the FastText embeddings. The GloVe embeddings were trained on the Common Crawl dataset, which is a dataset of web pages that is freely available. The FastText embeddings were trained on Wikipedia and Common Crawl. 

Here are two ways of accessing them. Be aware before running the lines below that the files are quite large and will take some time to download and unpack afterwards. For each session, you only need to download and import the files once. 

### FastText Embeddings

To download FastText, you can check out their [website](https://fasttext.cc/docs/en/crawl-vectors.html). They have off-the-shelf embeddings that you can download in 157 languages. To simply download embeddings in English, click [on this link](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz). Download your embeddings of choice to your own machine, preferably into the environment of your R-project. You can then import it. Since they are huge, I will do this step from my own hard drive. I use `data.table::fread()` here, because it is faster than `read_csv()`. Be prepared to wait a while, when downloading and importing the data. 

```{r}
fasttext <- data.table::fread("/Users/luissattelmayer/Downloads/cc.en.300.vec", 
                              quote ="", 
                              skip = 1, 
                              data.table = F, 
                              nrows = 5e5)
```


### GloVe Embeddings
In this example it is a bit easier to work with GloVe. To download the embeddings, you will need to first install the `text2map.pretrained` package directly from github using the `install_gitlab()` function from the `remotes` package. While this might seem a bit cumbersome, it is the only way to access the GloVe embeddings in R. But also, it is a good point to (finally) start using GitHub, to create an account, and eventually also a GitHub access token that allows you to install packages from GitHub. Without a Personal Access Token (PAT), you will not be able to install the `text2map.pretrained` package. Here is a [tutorial](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-personal-access-token-classic) on how to create a PAT. I recommend you store it in your `.Renviron` file. Here is a [tutorial](https://laurenilano.com/posts/api-keys/) on what that is and how to use it. Otherwise, I have also written a short tutorial on how to store tokens, access keys, and other confidential passwords in your `.Renviron` file in the script concerning the Spotify API. 

Now let's turn to the code. After you have installed the `text2map.pretrained` package, you can download the GloVe embeddings, you can use the following code to download the GloVe embeddings. 

```{r}
# remotes::install_gitlab("culturalcartography/text2map.pretrained", force = TRUE)
# library(text2map.pretrained)
# text2map.pretrained::download_pretrained("vecs_glove300_wiki_gigaword", force = TRUE)



data("vecs_glove300_wiki_gigaword")

```

The GloVe embeddings should be a large matrix with 120000000 elements. The name is very long and I rename it to `glove_wiki`. Lastly, I removed the original object to save memory.

```{r}
glove_wiki <- vecs_glove300_wiki_gigaword
rm(vecs_glove300_wiki_gigaword)
```


The `text2vec` package provides a function to calculate the cosine similarity between two words: `sim2`. It takes two words and the embeddings as arguments and returns the cosine similarity between the two words. 

```{r}
cos_sim <- sim2(
  x = glove_wiki,
  y = glove_wiki["climate", , drop = FALSE],
  method = "cosine"
) |> 
  enframe(name = "token", value = "cosine_sim") |> 
  mutate(cosine_sim = cosine_sim[,1]) |> 
  arrange(-cosine_sim) |> 
  slice_max(cosine_sim, n = 10)

# give lowest cosine_sim

cos_sim
```

Don't hesitate to play around with different words that you put into the `sim2()` function and see what the highest cosine similarities are.


## Word Algebra
One of the most fascinating things (in theory) about word embeddings is that they allow us to do word algebra. I added *in theory* in parentheses, because the results are not always as clear-cut as the logic of word embeddings promises. 
For that, we first need to transform the fasttext object into a more handy format. After transforming it into a tibble, we can rename the columns to `d1`, `d2`, ..., `d300`. This is because the fasttext embeddings have 300 dimensions. We can then transform the tibble into a matrix, where the rownames are the words.

```{r}
fasttext_tbl <- fasttext |> 
  as_tibble() |> 
  rename_with(\(x) str_c("d", 1:300), where(is.numeric))


fasttext_mat <- as.matrix(fasttext[, -1])
rownames(fasttext_mat) <- fasttext$V1
```


Below I create a function that shows the best match for a given term. The function takes two arguments: the term in question and the number of best matches you want to see. It then calculates the cosine similarity between the term in question and all the other terms in the fasttext matrix. Lastly, it extracts the $n$ highest cosine similarities and returns them in a tibble.

```{r}
## Function to get best matches
norms <- sqrt(rowSums(fasttext_mat^2)) # calculate length of vectors for normalization

best_match <- function(term, n = 5){
  x <- fasttext_mat[term, ]
  cosine <- (fasttext_mat %*% as.matrix(x)) / (norms * sqrt(sum(x^2))) # calculate cosine similarities between term in question and all the others
  best_n <- order(cosine, decreasing = T)[1:n] #extract highest n cosines
  tibble(
    word = rownames(fasttext_mat)[best_n], 
    cosine = cosine[best_n]
    )
}
```


Now we can test the embeddings by comparing different terms. For this demonstration, I chose the name "Theodor" and its variations in different languages to illustrate how embeddings can capture the similarity between different spellings of the same name while also reflecting language-specific nuances. This highlights the fascinating, yet static nature of word embeddings. Unlike Transformer models like BERT, which dynamically adjust the meaning of words based on their context, word embeddings are fixed and reflect the patterns present in the dataset they were trained on. A Transformer model might capture the similarity between the names more effectively because it accounts for contextual relationships. However, BERT is more complex to use and requires additional computational resources. With static embeddings like FastText or GloVe, each variation of "The/éodor/e" produces distinct results that depend on the original language context in which the word vector was created. This underscores both the strengths and limitations of static embeddings when dealing with language-specific or contextual subtleties.

```{r}
best_match("Theodor", 10)
best_match("Theodore", 10)
best_match("Théodore", 10)
```


What about the famous example of word embeddings?

$King - Man + Woman = Queen$


```{r}
semantic_algebra <- function(term_1, term_2, term_3, n = 5){
  search_vec <- glove_wiki[term_1, , drop = FALSE] - glove_wiki[term_2, , drop = FALSE] + glove_wiki[term_3, , drop = FALSE]
  sim2(x = glove_wiki, y = search_vec, method = "cosine") |> 
    enframe(name = "token", value = "cosine_sim") |> 
    mutate(cosine_sim = cosine_sim[,1]) |> 
    arrange(-cosine_sim) |> 
    filter(!token %in% c(term_1, term_2, term_3)) |> 
    slice_max(cosine_sim, n = n)
}

semantic_algebra("king", "man", "woman")
```



```{r}
semantic_algebra("paris", "france", "germany")
```

```{r}
semantic_algebra("sea", "water", "sand")
```

Why does this not work?

```{r}
semantic_algebra("Paris", "France", "ugadevinewvs")
```




## Investigating Bias
As I have explained in the lecture, word embeddings have been used to investigate biases in corpora. Let's look at how a simple example of this might look like. I use the gloVe embeddings that I have worked with above. Keep in mind that this is trained on Wikipedia and the Common Crawl dataset. This means that our corpus consists of Wikipedia entries and websites. The biases we are therefore investigating are "only" the biases you can find in these texts. If you were interested in studying for example gender or ethnic biases in the literature as [Laura Nelson](https://www.sciencedirect.com/science/article/pii/S0304422X21000115) has done, you would have to construct this corpus first. If you want to know how gender biases pan out in the newspapers reporting [see Benoît de Courson for a more sophisticated but great approach](https://osf.io/j7ydu), you would need newspaper*S* and train embeddings on this corpus. If you only take one newspaper in your corpus, you will also "only" investigate the gender bias in this specific newspaper.

Below I create a tibble that contains two columns. First `words_describing_men` contains words that clearly designate people as being of the male gender. The same applies for `words_describing_women` and the words this column contains. Next, using the `get_direction()` function of the `text2map` package I feed it an anchor which will be compared to the `glove_wiki` embeddings. This anchor is the tibble I have just created. The columns you want to use as anchors, should be solid juxtapositions. In our case we oppose two genders. Below, I use another example to detect associations between classes where I oppose words that are associated with wealth to words associated with poverty. Lastly, I create an object with different jobs of which I would like to know whether they are closer (in the embedding space and thus more associated in the corpus) to either words describing men or women. The closer they are to one end of the anchors, the harder the bias would be. The `sim2` function that we have seen above gives us the cosine similarity.  


```{r}
gender <- tibble(
  words_describing_men = c("man", "boy", "grandfather", "gentleman", "husband"),
  words_describing_women = c("woman", "girl", "grandmother", "lady", "wife")
)

sem_dir <- get_direction(anchors = gender, wv = glove_wiki)

jobs <- c("secretary", "carpenter", "nurse", "ceo", "doctor")

job_gender_bias <- sim2(sem_dir, glove_wiki[jobs, ], method = "cosine")

bias_tbl <- tibble(direction = job_gender_bias[1, ],
                   term = colnames(job_gender_bias))

bias_tbl
```
Negative values indicate that a word is more closely associated with words describing women, while positive values correspond to more "male" words. 

Below I reproduce the same thing with something relating to class. Admittedly, this is not the best operationalization of class but in a simple binary logic of juxtaposing words associated to wealth and poverty, we can already find something interesting.

```{r}
# Define socioeconomic anchor pairs
socioeconomic <- tibble(
  add = c("rich", "wealthy", "affluent", "luxurious", "elite"),
  subtract = c("poor", "impoverished", "destitute", "needy", "working-class")
)

# Calculate the socioeconomic direction
socio_dir <- get_direction(anchors = socioeconomic, wv = glove_wiki)

# Terms to analyze
terms <- c("banker", "teacher", "janitor", "entrepreneur", "plumber", "artist")

# Calculate cosine similarity to the socioeconomic direction
term_socio_bias <- sim2(socio_dir, glove_wiki[terms, ], method = "cosine")

# Create a bias table
bias_tbl_socio <- tibble(direction = term_socio_bias[1, ],
                         term = colnames(term_socio_bias))

bias_tbl_socio

```



```{r}
# Define socioeconomic anchor pairs
left_right <- tibble(
  add = c("socialdemocratic"),
  subtract = c("conservative")
)


# Calculate the socioeconomic direction
left_right_dir <- get_direction(anchors = socioeconomic, wv = glove_wiki)

# Terms to analyze
terms <- c("economy", "social", "environment", "climate", "immigration")

# Calculate cosine similarity to the socioeconomic direction
term_socio_bias <- sim2(socio_dir, glove_wiki[terms, ], method = "cosine")

# Create a bias table
bias_tbl_socio <- tibble(direction = term_socio_bias[1, ],
                         term = colnames(term_socio_bias))

bias_tbl_socio
```



## Reducing embedding dimensionality
Apart from cosine similarities, we are still wrangling with embeddings that are in multidimensional spaces that the human mind is unable to comprehend. For that, we need to reduce the dimensionalities. The simplest way would be to run *Principal Component Analyses* (PCA) on our vectors to reduce them two a two-dimensional -- and thus comprehensible -- space. 


Here I use composers, rappers, and jazz musicians to 
```{r}
music_query <- c("Bach", "Mozart", "Wagner", "Mahler",
                 "Kendrick", "Eminem", "Drake", 
                 "Coltrane", "Davis", "Ellington") |> 
  str_to_lower()

music_pca <- prcomp(glove_wiki[music_query, ]) |> 
  pluck("x") |> 
  as_tibble(rownames = NA) |> 
  rownames_to_column("name")

music_pca |> 
  ggplot() +
  geom_label_repel(aes(PC1, PC2, label = name))
```


```{r}
job_query <- c("Economist", "Sociologist", "Psychologist", "Anthropologist", 
               "Historian", "Geographer", "Archeologist", "Theologist", "Pastor",
               "Priest") |> 
  str_to_lower()


job_pca <- prcomp(glove_wiki[job_query, ]) |> 
  pluck("x") |> 
  as_tibble(rownames = NA) |> 
  rownames_to_column("name")

job_pca |> 
  ggplot() +
  geom_label_repel(aes(PC1, PC2, label = name))

```


## How to train embeddings yourself?
As mentioned earlier, you can also train your own word embeddings, which generally involves two approaches. The first is to train embeddings entirely from scratch using your own corpus. The second is to use pre-trained embeddings and fine-tune them on your specific dataset.

For training embeddings from scratch, having access to a very large and high-quality corpus is absolutely essential. While it is technically possible to train embeddings on a smaller corpus, the resulting embeddings would likely have low quality and limited utility. The effectiveness of word embeddings strongly depends on the size and quality of the training data.

If your focus is on a specific domain—such as the language used in certain newspapers, a particular professional field, or even niche topics—you might consider training your own embeddings. This would allow the embeddings to better capture domain-specific vocabulary, context, and nuances that may not be well-represented in general-purpose pre-trained models.

In the following code, I use the `wordsalad` package to train embeddings on the UK manifestos. For demonstration purposes, I use a small corpus, but in practice, it is recommended to use a much larger corpus to ensure the quality of the embeddings. Keep in mind that training embeddings on a large corpus can be computationally intensive and time-consuming.

```{r}
uk_manifestos <- read_csv(here::here("data/gbr_manifesto_corpus_clean.csv"))

uk_greens <- uk_manifestos |> 
    filter(partyname == "Green")

uk_labour <- uk_manifestos |>
    filter(partyname == "Lab")

uk_tories <- uk_manifestos |>
    filter(partyname == "Tories")

uk_ukip_reform <- uk_manifestos |>
    filter(partyname %in% c("UKIP", "Reform UK")) |> 
    mutate(partyname = "UKIP/Reform UK")

```


The `glove` function from the `wordsalad` package takes a character vector as input and returns a matrix with the embeddings. The `dim` argument specifies the dimension of the embeddings. Here, I only use 50 dimensions to speed up the process. The `wordsalad` package is a wrapper around the `text2vec` package. It provides access to various word embedding methods (GloVe, fasttext and word2vec) to extract word vectors using a unified framework to increase reproducibility and correctness.


```{r}
green_emb <- wordsalad::glove(uk_greens$text |> 
                               str_remove_all("[:punct:]") |> 
                               str_to_lower(), 
                             dim = 50)
green_emb_mat <- as.matrix(green_emb[, 2:51])
rownames(green_emb_mat) <- green_emb$tokens

lab_emb <- wordsalad::glove(uk_labour$text |> 
                            str_remove_all("[:punct:]") |> 
                            str_to_lower(), 
                          dim = 50)

lab_emb_mat <- as.matrix(lab_emb[, 2:51])
rownames(lab_emb_mat) <- lab_emb$tokens

tory_emb <- wordsalad::glove(uk_tories$text |> 
                             str_remove_all("[:punct:]") |> 
                             str_to_lower(), 
                           dim = 50)

tory_emb_mat <- as.matrix(tory_emb[, 2:51])
rownames(tory_emb_mat) <- tory_emb$tokens

ukip_reform_emb <- wordsalad::glove(uk_ukip_reform$text |> 
                                    str_remove_all("[:punct:]") |> 
                                    str_to_lower(), 
                                  dim = 50)
ukip_reform_emb_mat <- as.matrix(ukip_reform_emb[, 2:51])
rownames(ukip_reform_emb_mat) <- ukip_reform_emb$tokens
```

Once you have trained these word embeddings, you can use them for a variety of tasks. One particularly interesting application is to incorporate the embeddings into a supervised machine learning model, such as a Random Forest or a Neural Network, to predict party affiliation based on the text of the manifesto.

To do this, you would need labeled data where each manifesto text is associated with its respective party. The embeddings can be used to represent the text as input features for the model, capturing the semantic information encoded in the trained vectors. This approach enables the model to leverage the linguistic patterns and contextual relationships learned by the embeddings to improve its predictions.

## Concept Mover's Difference
Concept Mover's Distance classifies documents of any length along a continuous measure of engagement with a given concept of interest using word embeddings. The `text2map` package provides a function (`CMDist`). 

I calculate a conceptual closeness score for each manifesto document by comparing its term-document matrix (DTM) to pre-trained word vectors (in this case, GloVe Wiki) that define the semantic space for the chosen concepts. These scores quantify the semantic alignment of each document with the selected concepts.



```{r}
# text2map.pretrained::download_pretrained("vecs_glove300_wiki_gigaword")
# data("vecs_glove300_wiki_gigaword")


# glove_wiki <- vecs_glove300_wiki_gigaword
# rm(vecs_glove300_wiki_gigaword)

df_text <- uk_manifestos |>
    filter(partyname %in% c("Green", "Lab", "Tories", "UKIP", "Reform UK")) |>
    mutate(partyname = case_when(
        partyname %in% c("UKIP", "Reform UK") ~ "UKIP/Reform UK",
        TRUE ~ partyname
    )) |>
    group_by(doc_id) |>
    summarize(text = paste(text, collapse = ", "))

dtm_manifestos <- bind_rows(uk_greens, uk_labour, uk_tories, uk_ukip_reform) |> 
    unnest_tokens(token, text) |>
    count(token, doc_id) |> 
    cast_dtm(document = doc_id, term = token, value = n)
```

The boxplots below illustrate the distribution of Concept Mover's Distance (CMD) scores for each concept across different parties, highlighting the degree to which party manifestos emphasize or align with these concepts. By grouping and comparing CMD scores, the analysis sheds light on both inter-party and intra-party variability in manifesto focus.

However, the results should be interpreted with caution. CMD scores are relative, not absolute, measures of concept alignment and are influenced by the embeddings used. Additionally, the embeddings derived from the manifesto corpus are low-dimensional, which may limit the quality of the results and their ability to fully capture complex semantic relationships.


```{r}
doc_closeness <- CMDist(dtm = dtm_manifestos, 
                        cw = c("immigration", "economy", "environment"), 
                        wv = glove_wiki)

doc_closeness |> 
    mutate(doc = doc_id |> as.integer()) |> 
    left_join(bind_rows(uk_greens, uk_labour, uk_tories, uk_ukip_reform) |> 
                  select(-text)) |> 
    pivot_longer(immigration:environment, names_to = "concept", values_to = "cmd") |> 
    ggplot() +
    geom_boxplot(aes(cmd)) +
    facet_wrap(vars(concept, partyname))
```





